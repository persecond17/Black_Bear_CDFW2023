{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z4TVrvhj36f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e58484-248f-408d-a1e6-d1305dae6013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "import multiprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from csv import DictWriter\n",
        "from collections import Counter\n",
        "from dateutil import parser\n",
        "import warnings \n",
        "warnings.filterwarnings(action = 'ignore') \n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labeling from scratch\n",
        "\n",
        "Since we don't have any labeled data at the beginning of our analysis, we need to define the classes we're interested in and create initial labeled data from scratch."
      ],
      "metadata": {
        "id": "xwOKHiHtG4dS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining research classes for initial labeling\n",
        "\n",
        "* label = 0: not related to real bears\n",
        "* label = 1: related to real bears, and even related to encountering real bears\n",
        "* label = 2: related to real bears, but not related to encountering real bears\n",
        "\n",
        "Class 1 is the most important category as it will be used to analyze the frequency and geographical distribution of bear encounters. Class 2 can be used as a supplementary category to evaluate people's emotional responses to bear encounters."
      ],
      "metadata": {
        "id": "4qLa6v92A8OP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define reuseable variables and functions\n",
        "\n",
        "Based on the bear-encountering ratio displayed by [iNaturalist](https://github.com/persecond17/Black_Bear_CDFW2023/blob/main/social_media_NLP_project/step_2_preprocessing/3_visualizations.ipynb), we have a reference distribution to follow. If the input record is classified as class 1 and meets both the Geo Quota and seasonal quota requirements, then we can include it in the labeled dataset.\n"
      ],
      "metadata": {
        "id": "2OqOdzIj1hIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "geo_quota = {'Humboldt':220, 'Tulare':80, 'Los Angeles':80, 'Mariposa':70, \\\n",
        "             'El Dorado':60, 'Tuolumne':50, 'other':440}\n",
        "season_quota = {'spring':70, 'summer':330, 'autumn':430, 'winter':170}"
      ],
      "metadata": {
        "id": "9uBfSKTUEFj9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = [0, 'index','tweet_id','created_datetime','content','author_id','place_id',\n",
        "          'location','longitude','latitude','county','label','season']"
      ],
      "metadata": {
        "id": "vhILh7OqEkDG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def season_identify(date):\n",
        "    \"\"\"\n",
        "    Identify the posted season of record. \n",
        "    \"\"\"\n",
        "    month = int(date[5:7])\n",
        "    if month < 4: return 'spring'\n",
        "    elif month < 7: return 'summer'\n",
        "    elif month < 10: return 'autumn'\n",
        "    elif month <= 12: return 'winter'"
      ],
      "metadata": {
        "id": "NYgabREVG-kJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def county_identify(series):\n",
        "    \"\"\"\n",
        "    Count the number of occurrences of each county \n",
        "    in the series, and return the dictionary with \n",
        "    the count of each county.\n",
        "    \"\"\"\n",
        "    temp = {'Humboldt':0, 'Tulare':0, 'Los Angeles':0, 'Mariposa':0, \\\n",
        "            'El Dorado':0, 'Tuolumne':0, 'other':0}\n",
        "    for c in series:\n",
        "        if c == 'Humboldt': temp['Humboldt'] += 1\n",
        "        elif c == 'Tulare': temp['Tulare'] += 1\n",
        "        elif c == 'Los Angeles': temp['Los Angeles'] += 1\n",
        "        elif c == 'Mariposa': temp['Mariposa'] += 1\n",
        "        elif c == 'El Dorado': temp['El Dorado'] += 1\n",
        "        elif c == 'Tuolumne': temp['Tuolumne'] += 1\n",
        "        else: temp['other'] += 1\n",
        "    return temp"
      ],
      "metadata": {
        "id": "CMtr3D2uHmUl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def control_sampling_ratio(line, geo, season, labeled_df, geo_quota, season_quota):\n",
        "    \"\"\"\n",
        "    Check if a tweet is a duplicate and if it meets certain \n",
        "    quotas (based on geographical and seasonal information),\n",
        "    and returns the tweet if it does not violate any of conditions.\n",
        "    \"\"\"\n",
        "    geo_dict = Counter(labeled_df['county'])\n",
        "    season_dict = Counter(labeled_df['season'])\n",
        "\n",
        "    if line.tweet_id in labeled_df.tweet_id:\n",
        "        print('Duplicate!')\n",
        "        return None\n",
        "    if geo in geo_quota.keys():\n",
        "        if geo_dict[geo] >= geo_quota[geo]:\n",
        "            print('Geo Quota Full!')\n",
        "            return None\n",
        "    else:\n",
        "        if geo_dict['other'] >= geo_quota['other']:\n",
        "            print('Geo Quota Full!')\n",
        "            return None\n",
        "    if season_dict[season] >= season_quota[season]:\n",
        "        print('Seasonal Quota Full!')\n",
        "        return None\n",
        "    return line"
      ],
      "metadata": {
        "id": "W9o6d64wHmSQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_choose(target_df):\n",
        "    \"\"\"\n",
        "    Random choose a record and parse it's \n",
        "    content, geo, and season information.\n",
        "    \"\"\"\n",
        "    line = target_df.iloc[random.choice(target_df.index)]\n",
        "    tweet = line.content\n",
        "    geo = line.county\n",
        "    season = season_identify(line.created_datetime)\n",
        "    return line, tweet, geo, season"
      ],
      "metadata": {
        "id": "th_DmNsiHmNW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Unlabeled and Labeled Datasets\n",
        "\n",
        "Let's load the unlabeled data from [the previous step](https://github.com/persecond17/Black_Bear_CDFW2023/blob/main/social_media_NLP_project/step_3_data_annotation/1_filtering.ipynb), as well as a small labeled dataset to check the distribution and explore."
      ],
      "metadata": {
        "id": "O-3KyKyO1low"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_df = pd.read_csv('filtered.csv').reset_index().drop(columns=['Unnamed: 0', 'level_0'])\n",
        "target_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjpAZDTnHmPx",
        "outputId": "2a3090e0-e85a-4fd5-f5e7-8bd95d8def95"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 167202 entries, 0 to 167201\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   index             167202 non-null  int64  \n",
            " 1   tweet_id          167202 non-null  float64\n",
            " 2   created_datetime  167202 non-null  object \n",
            " 3   content           167202 non-null  object \n",
            " 4   author_id         167202 non-null  float64\n",
            " 5   place_id          162300 non-null  object \n",
            " 6   location          162223 non-null  object \n",
            " 7   longitude         167202 non-null  float64\n",
            " 8   latitude          167202 non-null  float64\n",
            " 9   county            167202 non-null  object \n",
            "dtypes: float64(4), int64(1), object(5)\n",
            "memory usage: 12.8+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_df = pd.read_csv('labeled.csv')\n",
        "print(f\"Currently, we have {len(labeled_df.loc[labeled_df['label']==1])} Class 1 records.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcnwXGmr1aSP",
        "outputId": "8b42ee1c-0ff2-4b07-d031-52670493ec60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently, we have 83 Class 1 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Sampling, Quota Checking, Labeling, and Saving Records\n",
        "\n",
        "In this section, I randomly selected records from the unlabeled dataset and manually labeled them. \n",
        "\n",
        "However, this process is time-consuming due to the low ratio of the target Class 1 tweets, which is **less than 5%** according to my statistics. Therefore, we aimed to collect as few manually labeled tweets as possible to trigger further automatic learning techniques."
      ],
      "metadata": {
        "id": "LI6AQ4171qf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def manually_labeling(label, line, geo, season, labeled_df, geo_quota, season_quota, titles):\n",
        "    \"\"\"\n",
        "    Write qualified tweets (labeled as 1 and satisfying \n",
        "    both the geographical and seasonal quotas, or \n",
        "    labeled as 0 or 2) to the labeled database.\n",
        "    \"\"\"\n",
        "    new_cols = pd.DataFrame({'label': label, 'season': season}, index=range(1))\n",
        "    if label == 1:\n",
        "        line = control_sampling_ratio(line, geo, season, labeled_df, geo_quota, season_quota)\n",
        "        if len(line) > 0:\n",
        "            row = pd.concat([line, new_cols])\n",
        "        else: row = dict()\n",
        "    else: \n",
        "        row = pd.concat([line, new_cols])\n",
        "\n",
        "    # Open CSV file in append mode\n",
        "    with open('labeled.csv', 'a') as f:\n",
        "        writer = DictWriter(f, fieldnames=titles)\n",
        "        # writer.writeheader() #only used in the 1st time\n",
        "        writer.writerow(dict(row))\n",
        "        print(\"Success saved!\")"
      ],
      "metadata": {
        "id": "5Clnm4WYHmKL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line, tweet, geo, season = random_choose(target_df)\n",
        "tweet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "yiOjvV090h8A",
        "outputId": "0b5ef300-30f9-4a8a-a636-9c8b0296a336"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It's wedding season and so excited for the big day for Eveleen + Gus.  And you gotta love Cali..how cool is it to have snow for your engagement pictures.  Simply beautiful pc | michellelacsonphotography @ Big Bear… https://t.co/qXrG6Kvw8J\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = 0 #0 or 1 or 2\n",
        "manually_labeling(label, line, geo, season, labeled_df, geo_quota, season_quota, titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfPuwf1lE3gC",
        "outputId": "a0660c50-9d0a-43be-f27f-7296481b6ac0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterative Semi-Supervised Learning: Spectral Clustering + Ada Boosting\n",
        "\n",
        "Let's mount the Google Drive folder to the Colab notebook and set the working directory to the \"My Drive\" folder first."
      ],
      "metadata": {
        "id": "JUVQ-W0jS1aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "%cd /content/drive/My Drive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLC3nGNbKsFe",
        "outputId": "6613188c-905e-427c-eb54-bb54f1310c4f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge and preprocess labeled datasets"
      ],
      "metadata": {
        "id": "JCBl0vxmWuCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_csv() -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Obtain a list of all CSV files in a given directory, \n",
        "    iterate through each CSV file and load it into a DataFrame. \n",
        "    Concatenate all the DataFrames into a single one. Parse dates \n",
        "    and remove any duplicate rows based on the tweet_id column.\n",
        "    \"\"\"\n",
        "    csv_files = glob.glob('*.csv')\n",
        "    df_list = []\n",
        "    for file in csv_files:\n",
        "        if file.startswith('sample_df'):\n",
        "            df = pd.read_csv(file)\n",
        "            df_list.append(df)\n",
        "\n",
        "    df = pd.concat(df_list, axis=0, ignore_index=True)\n",
        "    df['created_datetime'] = df['created_datetime'].apply(parser.parse)\\\n",
        "                                .apply(lambda x: x.strftime('%Y-%m-%d'))\n",
        "    df['created_datetime'] = pd.to_datetime(df['created_datetime'])\n",
        "    df.drop_duplicates(subset=['tweet_id'], inplace=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "AgqwWFCj37xk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean_and_tokenize(text: str) -> object:\n",
        "    '''\n",
        "    Preprocess the text data by removing URLs, hashtags, \n",
        "    punctuations, and extra spaces. Convert uppercase \n",
        "    letters to lowercase, tokenize the words, remove \n",
        "    stopwords, and perform text lemmatization.\n",
        "    '''\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'#\\w+|\\@\\w+', '', text)\n",
        "    text  = \"\".join((char if char.isalpha() else \" \") for char in text)\n",
        "    text = re.sub('[\\W_]+', ' ', text)\n",
        "    text = re.sub(' +', ' ',text)\n",
        "    text = text.strip()\n",
        "    text = text.lower()\n",
        "    text = nltk.word_tokenize(text)#re.split('\\W+', text)\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "\n",
        "    ps = nltk.WordNetLemmatizer()\n",
        "    text = [ps.lemmatize(w) for w in text]\n",
        "    merged_text = ' '.join(text)\n",
        "    return text, merged_text"
      ],
      "metadata": {
        "id": "xpDAtu2N5d89"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin the analysis, we will read all of the labeled datasets `labeled_df` as well as the unlabeled dataset `new`. Then we can obtain a preliminary understanding of the number of positively labeled tweets as well as the timeframe in which they were posted.\n",
        "\n",
        "Next, we will apply preprocessing techniques such as removing unnecessary parts, lowercasing, tokenizing, and lemmatization on tweets to ensure that the text data is clean and structured."
      ],
      "metadata": {
        "id": "rFhqdBl7ir0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_df = merge_csv()\n",
        "print(labeled_df.groupby('label')['label'].count())\n",
        "print(f\"Start from {min(labeled_df.created_datetime)} to {max(labeled_df.created_datetime)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q77KfRhM4yWP",
        "outputId": "c9a02b69-d3ad-4515-9d1c-5dbd51185c9a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "0    318\n",
            "1    122\n",
            "2     80\n",
            "Name: label, dtype: int64\n",
            "Start from 2010-06-07 00:00:00 to 2022-12-27 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_df['tokenized_tweets'] = labeled_df['content'].apply(lambda x: text_clean_and_tokenize(x))\n",
        "labeled_df[['tokenized', 'merged']] = labeled_df['tokenized_tweets'].apply(lambda x: pd.Series(x))\n",
        "labeled_df.info()"
      ],
      "metadata": {
        "id": "g2pO5d165grC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02252ff2-2e13-4215-df84-8e9543718782"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 520 entries, 0 to 519\n",
            "Data columns (total 13 columns):\n",
            " #   Column            Non-Null Count  Dtype         \n",
            "---  ------            --------------  -----         \n",
            " 0   tweet_id          520 non-null    float64       \n",
            " 1   created_datetime  520 non-null    datetime64[ns]\n",
            " 2   content           520 non-null    object        \n",
            " 3   author_id         520 non-null    float64       \n",
            " 4   place_id          515 non-null    object        \n",
            " 5   location          515 non-null    object        \n",
            " 6   longitude         520 non-null    float64       \n",
            " 7   latitude          520 non-null    float64       \n",
            " 8   county            520 non-null    object        \n",
            " 9   label             520 non-null    int64         \n",
            " 10  tokenized_tweets  520 non-null    object        \n",
            " 11  tokenized         520 non-null    object        \n",
            " 12  merged            520 non-null    object        \n",
            "dtypes: datetime64[ns](1), float64(4), int64(1), object(7)\n",
            "memory usage: 56.9+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new = pd.read_csv(\"filtered.csv\").drop(columns=['Unnamed: 0', 'index'])\n",
        "new['tokenized_tweets'] = new['content'].apply(lambda x: text_clean_and_tokenize(x))\n",
        "new[['tokenized', 'merged']] = new['tokenized_tweets'].apply(lambda x: pd.Series(x))\n",
        "new['created_datetime'] = pd.to_datetime(new['created_datetime'])\n",
        "new.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAoPaBOkEYxO",
        "outputId": "e400763a-02e3-4f06-91a6-8a6adebdeb59"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 167202 entries, 0 to 167201\n",
            "Data columns (total 12 columns):\n",
            " #   Column            Non-Null Count   Dtype         \n",
            "---  ------            --------------   -----         \n",
            " 0   tweet_id          167202 non-null  float64       \n",
            " 1   created_datetime  167202 non-null  datetime64[ns]\n",
            " 2   content           167202 non-null  object        \n",
            " 3   author_id         167202 non-null  float64       \n",
            " 4   place_id          162300 non-null  object        \n",
            " 5   location          162223 non-null  object        \n",
            " 6   longitude         167202 non-null  float64       \n",
            " 7   latitude          167202 non-null  float64       \n",
            " 8   county            167202 non-null  object        \n",
            " 9   tokenized_tweets  167202 non-null  object        \n",
            " 10  tokenized         167202 non-null  object        \n",
            " 11  merged            167202 non-null  object        \n",
            "dtypes: datetime64[ns](1), float64(4), object(7)\n",
            "memory usage: 15.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorizing Data using Pre-Trained [GloVe](https://nlp.stanford.edu/projects/glove/) Model\n",
        "\n",
        "In this chapter, we will be leveraging a pre-trained GloVe (Global Vectors for Word Representation) model to convert textual data into numerical vectors. GloVe is an unsupervised learning algorithm that generates word embeddings from a large corpus of text, enabling us to represent each word in our labeled tweets as a high-dimensional vector that reflects its meaning in the context of the corpus. These vectors can then be used for further analysis, such as spectral clustering and Ada Boosting.\n",
        "\n",
        "First, let'sload a pre-trained GloVe word embedding model and convert it to the word2vec format. **Glove.twitter.27B.200d.txt** is a pre-trained word embedding model that has been trained on a large corpus of Twitter data. It contains 27 billion tokens and 1.2 million vocabulary words, each with a 200-dimensional vector representation."
      ],
      "metadata": {
        "id": "MKoLYhHs6vpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"glove.twitter.27B.200d.txt\"\n",
        "tmp_file = get_tmpfile(filepath)\n",
        "_ = glove2word2vec(filepath, tmp_file)\n",
        "glove = KeyedVectors.load_word2vec_format(tmp_file, binary=False)"
      ],
      "metadata": {
        "id": "oFc001znFdVg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the issue of having limited labeled data with positive class 1, I will augment this class by copying its existing data to increase the overall size of the labeled dataset. This will help improve the model's ability to capture the underlying patterns in the data. \n",
        "\n",
        "The augmented labeled data will be shuffled and be used to create the training set, consisting of X_train and y_train."
      ],
      "metadata": {
        "id": "8soVmR6tp27a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.concat([labeled_df.loc[labeled_df['label'] != 1], \n",
        "                      labeled_df.loc[labeled_df['label'] == 1], \n",
        "                      labeled_df.loc[labeled_df['label'] == 1]])\\\n",
        "             .sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "X_train, y_train = train_df['merged'].tolist(), train_df['label'].tolist()\n",
        "print(len(X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cMZT49kA0hr",
        "outputId": "56853dbb-6c95-4a71-e83e-a3575b84a8e9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we will retrieve a sample of the unlabeled data to be used as the test set for labeling."
      ],
      "metadata": {
        "id": "9XMXy1eAru-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 6 # i can be any integer in range(0, len(new)//500)\n",
        "test_df = new.iloc[i*500: (i+1)*500]\n",
        "X_test = test_df['merged']\n",
        "print(len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOa8Gdm_nCl0",
        "outputId": "0db1e705-da04-4fef-d6ce-0a4f966c19f2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I will define a function to create a list of sentence vectors, and then use it to convert the tweets in both X_train and X_test into vectors."
      ],
      "metadata": {
        "id": "LzAJ_MhJsrTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding(X: list, glove: dict) -> list:\n",
        "    \"\"\"\n",
        "    Iterate through each sentence of the input, extract word \n",
        "    vectors from the pre-trained GloVe model for each word, \n",
        "    and compute the mean of these vectors to obtain the sentence \n",
        "    vector. If no words are found in GloVe for a sentence, a zero \n",
        "    vector is used instead. Return a corpus of sentence vectors.\n",
        "    \"\"\"\n",
        "    corpus = []\n",
        "    for sentence in X:\n",
        "        vectors = [glove[word] for word in sentence if word in glove]\n",
        "        if vectors:\n",
        "            sentence_vec = np.mean(vectors, axis=0)\n",
        "        else:\n",
        "            sentence_vec = np.zeros(glove.vector_size)\n",
        "        corpus.append(sentence_vec)\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "bNK56M_3NLO1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_vectors = embedding(X_test, glove)\n",
        "X_train_vectors = embedding(X_train, glove)"
      ],
      "metadata": {
        "id": "YUbni__HbAq1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectral Clustering: Select cluster labels with the highest proportion of class 1\n",
        "\n",
        "Spectral Clustering is powerful for capturing complex structures and patterns in data, including non-linear, non-convex, and irregularly shaped clusters. In addition, Spectral Clustering can effectively reduce the dimensionality of high-dimensional data and is robust to noise and initialization parameters. It is especially well-suited for text clustering, which often exhibits these characteristics. \n",
        "\n",
        "If you're interested in learning more about four popular clustering techniques from scratch, including K-means, K-means++, Spectral Clustering, and DBSCAN, as well as their respective advantages and limitations, and how to implement them using Python, please click the notebook: [Exploring Clustering Techniques from Scratch: K-means, K-means++, Spectral Clustering, and DBSCAN](https://github.com/persecond17/Black_Bear_CDFW2023/blob/main/social_media_NLP_project/step_3_data_transformation/Clustering_from_Scratch.ipynb).\n",
        "\n",
        "Here, I am going to use it to train on the training set and identify clusters that have a high ratio of class 1 tweets."
      ],
      "metadata": {
        "id": "yNTbddbqbYKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_spectral_labels(X_train_vectors: list, glove: dict, k: int, train_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Create and fit a Spectral Clustering object to the training set, \n",
        "    and cluster it into a specified number (k) of clusters. Add the \n",
        "    cluster labels as a column to the training dataframe, and return \n",
        "    the updated dataframe and the Spectral Clustering model.    \n",
        "    \"\"\"\n",
        "    spectral = SpectralClustering(n_clusters=k, affinity='nearest_neighbors', \n",
        "                                  assign_labels='kmeans')\n",
        "    spectral.fit(X_train_vectors)\n",
        "    labels = spectral.fit_predict(X_train_vectors)\n",
        "    train_df['cluster_label'] = labels\n",
        "    return train_df, spectral"
      ],
      "metadata": {
        "id": "WLfGUfpO6u84"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_fit_cluster(train_df: pd.DataFrame, k: int):\n",
        "    \"\"\"\n",
        "    Calculate the ratio of class 1 in each cluster and choose the \n",
        "    top 3 clusters with the highest ratio. If the ratio of a cluster \n",
        "    in these 3 chosen clusters is higher than the threshold of 0.95, \n",
        "    it will be added to a list and returned.\n",
        "    \"\"\"\n",
        "    cluster_df = train_df.groupby(train_df['cluster_label']).apply(lambda x: x)\n",
        "    results = defaultdict(list)\n",
        "    for cl in range(k):\n",
        "        pos_num = cluster_df.loc[(cluster_df['cluster_label'] == cl) & (cluster_df['label'] == 1)]['label'].sum()\n",
        "        pos_ratio = pos_num / cluster_df.loc[cluster_df['cluster_label']==cl]['label'].sum()\n",
        "        results[cl] = [pos_num, round(pos_ratio, 2)]\n",
        "    target = sorted(results.items(), key=lambda x: -x[1][1])[:3]\n",
        "    chosen_cluster_labels = [cluster[0] for cluster in target if cluster[1][1] > .95] \n",
        "    return cluster_df, chosen_cluster_labels"
      ],
      "metadata": {
        "id": "96K8Ro4cjTF7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's review the class 1 tweets in the chosen clusters to make sure it's quality and accuracy:"
      ],
      "metadata": {
        "id": "y8rt41i8wwIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, spectral = create_spectral_labels(X_train_vectors, glove, 10, train_df)\n",
        "cluster_df, chosen_cluster_labels = find_best_fit_cluster(train_df, 10)\n",
        "\n",
        "class_1_tweets = cluster_df.loc[(cluster_df['cluster_label'].isin(chosen_cluster_labels)) \\\n",
        "                                & (cluster_df['label'] == 1)]['content']\n",
        "for s in class_1_tweets[:20]:\n",
        "    print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEJqCL-UYSDX",
        "outputId": "e3f614ad-2284-4a8e-c498-2e9f90213249"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That is a beautiful bear...😎😎😎😎😎 https://t.co/Ucrw15Jp1F\n",
            "@JessWaltonYR Bear Claw\n",
            "That is a beautiful bear...😎😎😎😎😎 https://t.co/Ucrw15Jp1F\n",
            "A bRuin bear. In its natural habitat. Before it is slaughtered https://t.co/V18i7tM6UP\n",
            "pure_blitz mama bear with cub. #constellation #constellations #needlepushers #blitzicr #csun… https://t.co/vXIqhOx0BL\n",
            "@diannaaboo That’s a bear\n",
            "Who brought the bear\n",
            "@Nicoo_m19 Bear\n",
            "@Nicoo_m19 Bear\n",
            "@diannaaboo That’s a bear\n",
            "Beat the bear 😂☘️ https://t.co/xQpsr9VKqY\n",
            "@POWERMAYO @honeychild1229 Bear ! 🐻💚 https://t.co/7i7Tz6UIaJ\n",
            "A bRuin bear. In its natural habitat. Before it is slaughtered https://t.co/V18i7tM6UP\n",
            "@twm1962 In with the bear on this🐻\n",
            "I just saw a bear!\n",
            "@puurrraatt Omg big bear 🥺💞\n",
            "Beat the bear 😂☘️ https://t.co/xQpsr9VKqY\n",
            "Just saw bare bear balls at #Comicon2018 #poolpanic Don't tell my mum plz.\n",
            "@twm1962 In with the bear on this🐻\n",
            "@princess_jem4 Bear!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon analyzing the class 1 tweets in the selected clusters, it is evident that they are highly related to bear-encountering scenarios, indicating that our clustering algorithm has performed well.\n",
        "\n",
        "With this successful outcome, we can proceed to use the identified cluster labels to cluster the unlabeled data for further analysis."
      ],
      "metadata": {
        "id": "65nZKXZ-65ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_label = spectral.fit_predict(X_test_vectors)\n",
        "test_df['cluster_label'] = cluster_label"
      ],
      "metadata": {
        "id": "uwYpr2rd6vMR"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaBoost: Predict labels for tweets classified into the chosen clusters\n",
        "\n",
        "AdaBoost (Adaptive Boosting) is a powerful ensemble learning algorithm that combines weak learners to form a strong predictor. It works by iteratively training weak models on subsets of the data, where each model focuses on the instances that were previously misclassified. \n",
        "\n",
        "AdaBoost can handle complex non-linear relationships in the data, robustness to overfitting, and good generalization performance. \n",
        "\n",
        "Here, I will be using Ada Boosting to predict labels for tweets that have been classified into the selected clusters by Spectral Clustering. By doing so, we can narrow down the range of tweets that need to be manually reviewed."
      ],
      "metadata": {
        "id": "IwIvc1gFcVbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = test_df.reset_index()\n",
        "test_selected_clusters = test_df.loc[test_df['cluster_label'].isin(chosen_cluster_labels)]\n",
        "test_vectors_selected_clusters = [X_test_vectors[i] for i in test_selected_clusters.index.tolist()]"
      ],
      "metadata": {
        "id": "WylVf-eH7mEE"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtc = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "ada_boost = AdaBoostClassifier(base_estimator=dtc, learning_rate=1, \n",
        "                              n_estimators=200, random_state=42)\n",
        "ada_boost.fit(X_train_vectors, y_train)\n",
        "y_test = ada_boost.predict(test_vectors_selected_clusters)\n",
        "\n",
        "print(f\"The number of predicted class 1 tweets was found to be: {dict(Counter(y_test))[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6llrGTh5wUQ",
        "outputId": "4585293a-166f-4630-f630-062238b01126"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of predicted class 1 tweets was found to be: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual review of Class 1 tweets and saving them to the labeled dataset"
      ],
      "metadata": {
        "id": "aYKkbrjQcc4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_new_labeled_tweets(manual_class_1_tweets: list, \n",
        "                           test_selected_clusters: pd.DataFrame, \n",
        "                           labeled_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Reformat the new Class 1 tweets dataframe, \n",
        "    add it to the original labeled dataset.\n",
        "    \"\"\"\n",
        "    new_labeled_df = test_selected_clusters.loc[manual_class_1_tweets]\\\n",
        "                                          .drop(columns=['index', 'cluster_label', 'ada_label'])\n",
        "    new_labeled_df['label'] = len(new_labeled_df) * [1]\n",
        "    labeled_df = pd.concat([labeled_df, new_labeled_df])\n",
        "    return labeled_df"
      ],
      "metadata": {
        "id": "6ZSsp-v72Vxa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_selected_clusters['ada_label'] = y_test\n",
        "test_class_1_tweets = test_selected_clusters.loc[test_selected_clusters['ada_label']==1]['content'].tolist()\n",
        "test_class_1_indexes = test_selected_clusters.loc[test_selected_clusters['ada_label']==1].index\n",
        "\n",
        "for i in range(len(test_class_1_tweets)):\n",
        "    print(test_class_1_indexes[i], test_class_1_tweets[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziPhDPsRUBFW",
        "outputId": "5948f596-6dc2-439f-ca91-addfe9bbaca6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88 @PdCams @LivePDNation When they said Tahoe, we knew it would be a bear. 😎\n",
            "138 @MeredithFrost @CanonUSAimaging @leica_camera @NikonUSA @500px @SonyUK @SonyAlpha Bear Lake, Rocky Mountain National Park; The Gamble House main entrance looking out; Nico with a filter. All but Bear Lake taken with my iPhone. https://t.co/4ANvywmj55\n",
            "156 Late night boba runs with mama bear 🤗\n",
            "160 Bear ate four backyard chickens at a home in La Cañada. Another bear sighting in Glendale prompts warnings: http://t.co/NAVKOQA\n",
            "191 Does a bear shit in the woods? https://t.co/kFf6lwQDyY\n",
            "255 Does a bear shit in the woods? https://t.co/PpThTIjCiB\n",
            "284 the 1 // blackbear\n",
            "365 Hiked #SunsetTrail today in the sequoianationalforest . Saw a big #blackbear about 40 feet from us while we were walking back &amp; lived to tell about it. I’m here for a reason. Leggoooo. 🐻… https://t.co/SpXPt0VaIc\n",
            "370 Monterey Update: Bull &amp; Bear is white washed because of car week\n",
            "410 Blackbear really is the most slept on\n",
            "417 ...mountains, they risk a deadly confrontation with the bear whose roar can shake the snow from the trees.” - #MTGM19 Player’s Guide. Art by @velinovart.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "manual_class_1_tweets = [88, 160, 191, 255, 284, 365, 410, 417]\n",
        "labeled_df = add_new_labeled_tweets(manual_class_1_tweets, test_selected_clusters, labeled_df)\n",
        "labeled_df['tweet_id'].count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GflnWF2FUuhN",
        "outputId": "2f028ca5-54b6-487b-f57b-80fe84150beb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "528"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge all steps together"
      ],
      "metadata": {
        "id": "DKdOqmed5euR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semi_supervised_learning(labeled_df: pd.DataFrame, new: pd.DataFrame, i: int, k: int, glove: dict):\n",
        "    \"\"\"\n",
        "    augment class 1 records in the training set\n",
        "    \"\"\"\n",
        "    # Define the training set\n",
        "    train_df = pd.concat([labeled_df.loc[labeled_df['label'] != 1], \n",
        "                      labeled_df.loc[labeled_df['label'] == 1], \n",
        "                      labeled_df.loc[labeled_df['label'] == 1]])\\\n",
        "             .sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    X_train, y_train = train_df['merged'].tolist(), train_df['label'].tolist()\n",
        "    # Extract the test set\n",
        "    test_df = new.iloc[i*500: (i+1)*500]\n",
        "    X_test = test_df['merged']\n",
        "    # Convert text data to vectors\n",
        "    X_test_vectors = embedding(X_test, glove)\n",
        "    X_train_vectors = embedding(X_train, glove)\n",
        "\n",
        "    # Apply spectral Clustering on the training set \n",
        "    # and find the cluster with high ratio of Class 1 tweets\n",
        "    train_df, spectral = create_spectral_labels(X_train_vectors, glove, k, train_df)\n",
        "    cluster_df, chosen_cluster_labels = find_best_fit_cluster(train_df, k)\n",
        "    # Perform spectral Clustering on the test set\n",
        "    # and retrieve tweets in the chosen clusters\n",
        "    cluster_label = spectral.fit_predict(X_test_vectors)\n",
        "    test_df['cluster_label'] = cluster_label\n",
        "    test_df = test_df.reset_index()\n",
        "    test_selected_clusters = test_df.loc[test_df['cluster_label'].isin(chosen_cluster_labels)]\n",
        "    test_vectors_selected_clusters = [X_test_vectors[i] for i in test_selected_clusters.index.tolist()]\n",
        "\n",
        "    # Conduct AdaBoost on the retrieved tweets\n",
        "    dtc = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "    ada_boost = AdaBoostClassifier(base_estimator=dtc, learning_rate=1, \n",
        "                                  n_estimators=200, random_state=42)\n",
        "    ada_boost.fit(X_train_vectors, y_train)\n",
        "    y_test = ada_boost.predict(test_vectors_selected_clusters)\n",
        "\n",
        "    # Return Class 1 tweets in the chosen clusters for manually labeling\n",
        "    test_selected_clusters['ada_label'] = y_test\n",
        "    test_class_1_tweets = test_selected_clusters.loc[test_selected_clusters['ada_label']==1]['content'].tolist()\n",
        "    test_class_1_indexes = test_selected_clusters.loc[test_selected_clusters['ada_label']==1].index\n",
        "\n",
        "    return test_class_1_tweets, test_class_1_indexes, test_selected_clusters"
      ],
      "metadata": {
        "id": "TmoUYCuI5d-o"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's apply the function on another set of unlabeled data:"
      ],
      "metadata": {
        "id": "gsswTyKy_yBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_class_1_tweets, test_class_1_indexes, test_selected_clusters = semi_supervised_learning(labeled_df, new, 12, 10, glove)\n",
        "for i in range(len(test_class_1_tweets)):\n",
        "    print(test_class_1_indexes[i], test_class_1_tweets[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQN8jf4201ya",
        "outputId": "6c9bbe28-467b-44a2-a241-aeb60994b1ca"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13 @desterella Ima facetime you in the morning when she wake up &amp; mama bear at work lol\n",
            "40 @i_am_BMW It’s only a 5ft bear\n",
            "43 @MsAutumnWind I read “Brown Bear Brown Bear” and”Polar Bear Polar Bear” to my granddaughter rather than watch the second half🤮\n",
            "69 @GeoHolms “I don’t speak bear!”\n",
            "115 Brother bear is having a kickback and all I hear is bass against my wall. 🙃\n",
            "179 “ I need a bear one that does AHHHHHH” 😂😂😂😂😂 https://t.co/Y6UMYn6abB\n",
            "192 @robkahl The mama bear is Cardinals in this comp\n",
            "213 @StormHuntley @ranee_11 BABY BEAR MADE IT AHHHH https://t.co/EgBi4WDGed\n",
            "269 “@caylacruz: @NikkMode fuck, it must!!! Hahaha by a bear or something #raaaahrrrr” lmao @ raaaahrrr\n",
            "285 Dinner with my Mar Bear!! @ Eat Chow https://t.co/Am24FgvNEt\n",
            "288 @poetpooch @andrealori @AMHighcrest @HRHLou @joancbaez oh, wow, you are in bear country! Are these brown or black bears?\n",
            "413 Also there’s a bear https://t.co/7ZmbRcQM5U\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "manual_class_1_tweets = [40, 213, 288, 413]\n",
        "labeled_df = add_new_labeled_tweets(manual_class_1_tweets, test_selected_clusters, labeled_df)\n",
        "labeled_df['tweet_id'].count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwtipPwC_uOL",
        "outputId": "8f12ad02-61c9-49d3-9ce5-3ccc7ef633c8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "532"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jXsNGoGTgyKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}